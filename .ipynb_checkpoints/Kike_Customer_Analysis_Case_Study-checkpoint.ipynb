{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Modelling\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import (StandardScaler, MinMaxScaler)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from scipy.stats import iqr\n",
    "from scipy.stats import scoreatpercentile as pct\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import RFE\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Pipeline Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Wrapping the loading operation - For Pipeline Control\n",
    "\n",
    "def load_data():\n",
    "\n",
    "    file1 = pd.read_csv(\"Data/file1.csv\")\n",
    "    file2 = pd.read_csv(\"Data/file2.csv\")\n",
    "    file3 = pd.read_csv(\"Data/file3.csv\")\n",
    "\n",
    "    file3.rename(columns={\"State\":\"ST\", \"Gender\": \"GENDER\"}, inplace=True)\n",
    "\n",
    "    return pd.concat([file1,file2,file3]).reset_index(drop=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing Headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_headings(df):\n",
    "    \"\"\"\n",
    "    Returns a Pandas Dataframe with an standarized heading, i.e lower case and \" \" replaced by \"_\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    columns_list : Pandas Dataframe\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Returns a Pandas Dataframe with an standarized heading, i.e lower case and \" \" replaced by \"_\"\n",
    "\n",
    "    \"\"\"  \n",
    "    heading = df.columns\n",
    "    df.columns = [clabel.lower().replace(\" \", \"_\") for clabel in heading]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Wrapping the two data correction type operations - For Pipeline Control\n",
    "\n",
    "def dt_corrector(df):\n",
    "    \n",
    "    df.customer_lifetime_value = df.customer_lifetime_value.apply(lambda x: np.round(float(x.strip(\"%\"))*100) if type(x)==str else np.round(x) if x==x else x)\n",
    "    df.number_of_open_complaints = df.number_of_open_complaints.apply(lambda x: np.array(x.split(\"/\"), dtype=int)[1] if type(x)==str else x)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bucket_series(df, columns_list, bucketing_dict):\n",
    "    \"\"\"\n",
    "    Returns a Pandas Dataframe with the columns contained in columns_list modified to only have unique labels. The reduction is done following the logic contained in cleaning_dict.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Pandas Dataframe\n",
    "        The data frame to be cleaned\n",
    "    columns_list : List\n",
    "        A list of strings with the labels of the target columns\n",
    "    cleaning_dict : Dictionary\n",
    "        A dictionary of the form {redundant_value : unique_value}\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    Returns a Pandas Dataframe with the columns contained in columns_list modified to only have unique labels. The reduction is done following the logic contained in cleaning_dict.\n",
    "\n",
    "    \"\"\"\n",
    "    for column in columns_list:\n",
    "\n",
    "        df[column] = list(map(lambda item : bucketing_dict[item] if item in bucketing_dict.keys() else item, df[column]))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing Null values (Numeric Variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_by(df, columns_list, replace_zeros = False, replacer = np.mean):\n",
    "    \"\"\"\n",
    "    Returns a Pandas Dataframe with the target columns nan values replaced by the column selected paramete (median or mean).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Pandas dataframe\n",
    "        The data frame to be cleaned\n",
    "    columns_list : List\n",
    "        A list of strings with the labels of the target columns\n",
    "\n",
    "    parameter : String\n",
    "        Parameter to be used for the replacement\n",
    "        \"mean\" for the mean (default)\n",
    "        \"median\" for the median\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Returns a Pandas Dataframe with the target columns nan values replaced by the column selected paramete (median or mean).\n",
    "\n",
    "    \"\"\"\n",
    "    for column in columns_list:\n",
    "\n",
    "        if replace_zeros == True:\n",
    "\n",
    "            target = df[column][df[column] > 0]\n",
    "            df[column] = list(map(lambda x: replacer(target) if x == 0 else replacer(target) if x!=x else x, df[column]))\n",
    "\n",
    "        elif replace_zeros == False:\n",
    "\n",
    "            target = df[column]\n",
    "            df[column] = df[column].fillna(replacer(target))\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing Null values (Categorical Variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_by_cat(df, columns_list):\n",
    "    \n",
    "    for column in columns_list:\n",
    "\n",
    "        if column[1] == \"mode\":\n",
    "\n",
    "            replacer = mode(df[column[0]])\n",
    "        \n",
    "        else:\n",
    "\n",
    "            replacer = column[1]\n",
    "\n",
    "\n",
    "        df[column[0]] = list(map(lambda x:replacer if x != x else x, df[column[0]]))\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Float Columns to Int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_to_int(df, columns_list):\n",
    "    \"\"\"\n",
    "    Returns a Pandas Dataframe with target columns casted from float to int.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Pandas Dataframe\n",
    "        \n",
    "    columns_list : List\n",
    "        A list of strings with the labels of the target columns\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    Returns a Pandas Dataframe with target columns casted from float to int.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    for column in columns_list:\n",
    "\n",
    "        df[column] = df[column].astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Obj Columns to Cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_to_cat(df, columns_list):\n",
    "\n",
    "    for column in columns_list:\n",
    "\n",
    "        df[column] = df[column].astype(\"category\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(df):\n",
    "\n",
    "    df_standardized = pd.DataFrame(list(map(lambda x: list(map(lambda y: y.lower() if type(y)==str else y, x)), df.values)))\n",
    "    df_standardized.columns = df.columns\n",
    "\n",
    "    return df_standardized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to store in dictionary the number of nan values per column\n",
    "\n",
    "def nan_counter(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a dictionary containing the number of nan values per column (for dataframe df)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Pandas dataframe\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    remaining_nan : Dictionary\n",
    "        Contains the number of nan values in each column of the dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    remaining_nan = {}\n",
    "\n",
    "    for column in df.columns:\n",
    "\n",
    "        remaining_nan[column] = df[column][df[column].isna() == True].size\n",
    "\n",
    "    return remaining_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_rate_vs(df, variable, grouper):\n",
    "    \"\"\"\n",
    "    Returns a panda Series with:\n",
    "    Index: the categories inside the grouper\n",
    "    Values: the variable (for example Response) rate for the group\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Pandas Dataframe\n",
    "        DESCRIPTION.\n",
    "    variable : String\n",
    "        The label of the df column containing the variable data used to calculate the rate, e.g Response\n",
    "    grouper : String\n",
    "        The label of the df column used to group the variable rate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Panda Series\n",
    "        Index: the categories inside the grouper\n",
    "        Values: the variable (for example Response) rate for the group\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Dictionary to store the results\n",
    "    rate_dict = {}\n",
    "    \n",
    "    # Series and group seting, they will be used later in the for loop for indexing and accessing the data\n",
    "    var_cats = df[variable].value_counts()\n",
    "    group_cats = df[grouper].value_counts()\n",
    "    group = df.groupby([grouper, variable])[variable].count()\n",
    "    \n",
    "    # The outer for loops through the groups\n",
    "    ## The inner for loops the group series and for the specific group access the variable catergories and stores it in a list\n",
    "    ## At the end of the innerr loop the rate for the group is stored in a dict of the form {group:rate}\n",
    "    \n",
    "    for g in group_cats.index:\n",
    "\n",
    "        store = []\n",
    "\n",
    "        for c in var_cats.index:\n",
    "        \n",
    "            n = group.loc[(g, c)]\n",
    "            store.append(n)\n",
    "\n",
    "        rate_dict[g] = round((store[1] / store[0])*100, 2)\n",
    "\n",
    "    # A series is returned\n",
    "    return pd.Series(rate_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = [\"customer\"]\n",
    "bucket_columns = [\"st\", \"gender\"]\n",
    "clean_columns_NaN = [\"customer_lifetime_value\", \"income\", \"monthly_premium_auto\", \"number_of_open_complaints\", \"total_claim_amount\"]\n",
    "clean_columns_zeros = [\"income\"]\n",
    "clean_columns_cat = [(\"gender\", \"U\")]\n",
    "float_columns = [\"customer_lifetime_value\", \"income\", \"monthly_premium_auto\", \"number_of_open_complaints\", \"total_claim_amount\"]\n",
    "obj_columns = [\"st\", \"gender\", \"education\", \"number_of_open_complaints\", \"policy_type\", \"vehicle_class\"]\n",
    "\n",
    "bucketing_dict = {\"F\":\"F\", \"female\":\"F\", \"Femal\":\"F\"\n",
    "                 ,\"M\":\"M\", \"Male\":\"M\"\n",
    "                 ,\"California\": \"California\", \"Cali\": \"California\"\n",
    "                 ,\"Arizona\":\"Arizona\",\"AZ\":\"Arizona\"\n",
    "                 ,\"Washington\":\"Washington\", \"WA\":\"Washington\"\n",
    "                 ,\"Oregon\":\"Oregon\"\n",
    "                 ,\"Nevada\":\"Nevada\"\n",
    "                 ,np.nan:np.nan\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/file1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x7/r2pcnk8558z12w_v99lrx7z00000gn/T/ipykernel_88468/3981278930.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m ca_df = (load_data()\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstandard_headings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt_corrector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket_series\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucketing_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/x7/r2pcnk8558z12w_v99lrx7z00000gn/T/ipykernel_88468/1141218971.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfile1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data/file1.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mfile2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data/file2.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mfile3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data/file3.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/DA_ENV/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/DA_ENV/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/DA_ENV/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/DA_ENV/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/DA_ENV/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/DA_ENV/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/DA_ENV/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/DA_ENV/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/file1.csv'"
     ]
    }
   ],
   "source": [
    "ca_df = (load_data()\n",
    ".pipe(standard_headings)\n",
    ".drop(drop_columns, axis=1)\n",
    ".pipe(dt_corrector)\n",
    ".pipe(bucket_series, bucket_columns, bucketing_dict)\n",
    ".drop_duplicates().reset_index(drop=True)\n",
    ".pipe(replace_by, clean_columns_NaN)\n",
    ".pipe(replace_by, clean_columns_zeros)\n",
    ".pipe(replace_by_cat, clean_columns_cat)\n",
    ".dropna()\n",
    ".pipe(standardize_data)\n",
    ".pipe(float_to_int, float_columns)\n",
    ".pipe(obj_to_cat, obj_columns)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = pd.read_csv(\"Data/file1.csv\")\n",
    "file2 = pd.read_csv(\"Data/file2.csv\")\n",
    "file3 = pd.read_csv(\"Data/file3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the imported data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's check if the shape of the 3 files is compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The shape of file1 is {file1.shape}')\n",
    "print(f'The shape of file2 is {file2.shape}')\n",
    "print(f'The shape of file3 is {file3.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are. Now let's check if the headings are the same in order to avoid generating extra columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(file1.columns) ^ set(file2.columns))\n",
    "print(set(file1.columns) ^ set(file3.columns))\n",
    "print(set(file2.columns) ^ set(file3.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before concatenaiting the data I will change the heading of file3 to be the same as in file1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file3.rename(columns={\"State\":\"ST\", \"Gender\": \"GENDER\"}, inplace=True)\n",
    "\n",
    "ca_df = pd.concat([file1,file2,file3]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check everything looks right in terms of importing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardizing Headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ca_df = standard_headings(ca_df)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deleting and rearranging columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the column customer, as we dont need it for our analysis. We wrap the operation in a function. The function will be used in the Pipeline Controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_df = ca_df.drop([\"customer\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check how the dataframe looks now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correcting data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's show the datatypes we have in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer_lifetime_value and number_of_open_complaints should be numeric. Let's take a deeper look into these two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set([type(item) for item in ca_df.customer_lifetime_value]))\n",
    "print(set([type(item) for item in ca_df.number_of_open_complaints]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert into float/integer the strings. In addition:\n",
    "\n",
    "- Customer_lifetime_value: some of the data ends with symbol %\n",
    "- number_of_open_complaints: some of the data is presented in the form n1/n2/n3. Analyzing it we can figure out that the number of complaints is actually n2 (n1 seems to be a control number taking only values 0 or 1 and n3 is always 0)\n",
    "\n",
    "The option of mapping a function in the corresponding data series has been explored below. I will use a lambda function instead of a dedicated function because the purpose of the function is very specific and it is unlikely to be reused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the value is a string\n",
    "## strip the symbol %, convert into float and round to 0 decimals\n",
    "## Else check if the valu is Nan\n",
    "### if yes return value\n",
    "### if not round to 0 decimals\n",
    "\n",
    "ca_df.customer_lifetime_value = ca_df.customer_lifetime_value.apply(lambda x: np.round(float(x.strip(\"%\"))*100) if type(x)==str else np.round(x) if x==x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the value is a string of the form n/n/n\n",
    "## Split it into a list, convert it into array of int and apply sum()\n",
    "## Else return the value\n",
    "\n",
    "ca_df.number_of_open_complaints = ca_df.number_of_open_complaints.apply(lambda x: np.array(x.split(\"/\"), dtype=int)[1] if type(x)==str else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering data and Correcting typos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns gender and st have obvious problems that need to be cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ca_df.gender.value_counts(dropna=False))\n",
    "print()\n",
    "print(ca_df.st.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- column st: we will reduce the data to the following unique values [Washington, California, Arizona, Oregon, Nevada]\n",
    "- column gender: we will reduce the data to the following unique values [M, F]\n",
    "\n",
    "For this we create a clean_series() function and a clean_one_series() helper function. I decided to use generic functions here because I think they could be used in the future for other projects. The version with a dedicated functions is in a commented cell at the end of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_dict = {\"F\":\"F\", \"female\":\"F\", \"Femal\":\"F\", \n",
    "                 \"M\":\"M\", \"Male\":\"M\", \n",
    "                 \"California\": \"California\", \"Cali\": \"California\", \n",
    "                 \"Arizona\":\"Arizona\",\"AZ\":\"Arizona\", \n",
    "                 \"Washington\":\"Washington\", \"WA\":\"Washington\",\n",
    "                 \"Oregon\":\"Oregon\",\n",
    "                 \"Nevada\":\"Nevada\",\n",
    "                 np.nan:np.nan\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ca_df = bucket_series(ca_df, [\"gender\", \"st\"], cleaning_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ca_df.gender.value_counts(dropna=False))\n",
    "print()\n",
    "print(ca_df.st.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we remove duplicate row.\n",
    "There were (12074 - 8876) = 3198 duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_df.drop_duplicates(inplace=True)\n",
    "ca_df = ca_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ca_df.gender.value_counts(dropna=False))\n",
    "print()\n",
    "print(ca_df.st.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacing null values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the numeric values we will replace nan values with the mean. The following functions have been created:\n",
    "\n",
    "- replace_by_mean()\n",
    "- nan_counter() -only for checking purposes-\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list = [\"customer_lifetime_value\", \"income\", \"monthly_premium_auto\", \"number_of_open_complaints\", \"total_claim_amount\"]\n",
    "ca_df = replace_by(ca_df, columns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nan_counter(ca_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the income column?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The income columns has more than 2,000 values=0. Does that make sense? Not really"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_df.income[ca_df.income == 0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_df.income.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us replace the zeros by the Income mean (calculated exlcuding zeros)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_df = replace_by(ca_df, [\"income\"], replace_zeros=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_df.income[ca_df.income == 0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_df.income.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we don't have Nan values we can convert floats to integer if appropiate. In this case all the numeric columns can be converted to int without losing significant precision. Again we define a function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_to_int(ca_df, columns_list)\n",
    "ca_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nan value rows dropped in those column with only one occurrence. In the gender column Nan values replace by \"U\" (for Unknown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nan_counter(ca_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_df = replace_by_cat(ca_df, [(\"gender\", \"U\")])\n",
    "ca_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nan_counter(ca_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how the data looks now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_df_standarized = standardize_data(ca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_df_standarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the csv file with all the data (provided on Wednesday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_c_df = pd.read_csv(\"Data/Data_Marketing_Customer_Analysis_Round2.csv\")\n",
    "ca_c_df = standard_headings(ca_c_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = [\"unnamed:_0\", \"customer\", \"effective_to_date\", \"policy\", \"vehicle_type\"]\n",
    "bucket_columns = [\"vehicle_class\"]\n",
    "# clean_columns_NaN = [\"customer_lifetime_value\", \"income\", \"monthly_premium_auto\", \"number_of_open_complaints\", \"total_claim_amount\"]\n",
    "clean_columns_zeros = [\"income\"]\n",
    "clean_columns_cat = [(\"response\", \"No\")]\n",
    "float_columns = list(set(ca_c_df._get_numeric_data().columns).difference(set(drop_columns)))\n",
    "obj_columns = list(set(ca_c_df.select_dtypes(\"object\").columns).difference(set(drop_columns)))\n",
    "\n",
    "cleaning_dict = {\"Luxury SUV\":\"Luxury\", \"Luxury Car\":\"Luxury\"\n",
    "                 ,np.nan:np.nan\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_c_df = (pd.read_csv(\"Data/Data_Marketing_Customer_Analysis_Round2.csv\")\n",
    ".pipe(standard_headings)\n",
    ".drop(drop_columns, axis=1)\n",
    "#.pipe(dt_corrector)\n",
    ".pipe(bucket_series, bucket_columns ,cleaning_dict)\n",
    "#.drop_duplicates().reset_index(drop=True)\n",
    "#.pipe(replace_by, clean_columns_NaN)\n",
    ".pipe(replace_by, clean_columns_zeros)\n",
    ".pipe(replace_by_cat, clean_columns_cat)\n",
    ".dropna()\n",
    ".pipe(standardize_data)\n",
    ".pipe(float_to_int, float_columns)\n",
    ".pipe(obj_to_cat, obj_columns)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_c_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_c_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_counter(ca_c_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of the total number of responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new variable with the categories in Respone column [Yes, No]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ca_c_df[\"response\"].value_counts(dropna=False)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code plots and shows the total response as a bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theme\n",
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure and axes\n",
    "fig1, ax1 = plt.subplots(figsize = (8,8))\n",
    "\n",
    "# set title\n",
    "ax1.set_title('total_response_number', fontweight='bold')\n",
    "\n",
    "# plot the bar chart\n",
    "ax1.bar(response.index, response, color=('red','green', 'black'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of the response rate by the sales channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_channel_rate = variable_rate_vs(ca_c_df, \"response\", \"sales_channel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure and axes\n",
    "fig2, ax2 = plt.subplots(figsize = (8,8))\n",
    "\n",
    "# set title\n",
    "ax2.set_title('Response Rate by Channel', fontweight='bold')\n",
    "\n",
    "# axes titles\n",
    "ax2.set_xlabel(\"Channel\", fontweight='bold')\n",
    "ax2.set_ylabel(\"Response Rate (%)\", fontweight='bold')\n",
    "\n",
    "# plot the bar chart\n",
    "ax2.bar(response_channel_rate.index, response_channel_rate, color=('blue','orange', 'yellow', \"red\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure and axes\n",
    "fig2b, ax2b = plt.subplots(figsize = (8,8))\n",
    "\n",
    "# set title\n",
    "ax2b.set_title('Response Rate by Channel', fontweight='bold')\n",
    "\n",
    "# plot the bar chart\n",
    "ax2b = sns.countplot(\"sales_channel\", hue=\"response\", data = ca_c_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code plots and shows the total response as a bar plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of the response rate by the Total Claim Amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I will bin the Total Claim Amount into categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_claim_amount_labels = ['Low', 'Moderate', 'High']\n",
    "ca_c_df['total_claim_amount_binned'] = pd.cut(ca_c_df['total_claim_amount'],3, labels=total_claim_amount_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_c_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then fuction variable_rate_vs() can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_total_claim_amount_rate = variable_rate_vs(ca_c_df, \"response\", \"total_claim_amount_binned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_total_claim_amount_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code plots and shows the total response as a bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure and axes\n",
    "fig3, ax3 = plt.subplots(figsize = (8,8))\n",
    "\n",
    "# set title\n",
    "ax3.set_title('Response Rate by Total Claim Amount', fontweight='bold')\n",
    "\n",
    "# axes titles\n",
    "ax3.set_xlabel(\"Total Claim Amount\", fontweight='bold')\n",
    "ax3.set_ylabel(\"Response Rate (%)\", fontweight='bold')\n",
    "\n",
    "# plot the bar chart\n",
    "ax3.bar(response_total_claim_amount_rate.index, response_total_claim_amount_rate, color=('blue','orange', 'yellow', \"red\", \"black\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure and axes\n",
    "fig3b, ax3b = plt.subplots(figsize = (8,8))\n",
    "\n",
    "# set title\n",
    "ax3b.set_title('Response Rate by Total Claim Amount', fontweight='bold')\n",
    "\n",
    "# plot the bar chart\n",
    "ax3b = sns.countplot(\"total_claim_amount_binned\", hue=\"response\", data = ca_c_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us do some checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_c_df[\"response\"][ca_c_df.total_claim_amount_binned == (\"High\" or \"Very High\")].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high and very high claims are not responded, which kind of makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of the response rate by income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach is very simmilar to that for the income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_labels = ['Very Low', 'Low', 'Moderate', 'High', 'Very High']\n",
    "ca_c_df['income_binned'] = pd.cut(ca_c_df['income'],5, labels=income_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_c_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_income_rate = variable_rate_vs(ca_c_df, \"response\", \"income_binned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_income_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure and axes\n",
    "fig4, ax4 = plt.subplots(figsize = (8,8))\n",
    "\n",
    "# set title\n",
    "ax4.set_title('Response Rate by Income', fontweight='bold')\n",
    "\n",
    "# axes titles\n",
    "ax4.set_xlabel(\"Income\", fontweight='bold')\n",
    "ax4.set_ylabel(\"Response Rate (%)\", fontweight='bold')\n",
    "\n",
    "# plot the bar chart\n",
    "ax4.bar(response_income_rate.index, response_income_rate, color=('blue','orange', 'yellow', \"red\", \"black\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure and axes\n",
    "fig4b, ax4b = plt.subplots(figsize = (8,8))\n",
    "\n",
    "# set title\n",
    "ax4b.set_title('Response Rate by Income', fontweight='bold')\n",
    "\n",
    "# plot the bar chart\n",
    "ax4b = sns.countplot(\"income_binned\", hue=\"response\", data = ca_c_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the income does not correlate with the response rate. Before the analysis I would have assumed that the claims of the High-Very High income clients will be responded more often. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Correlation Heatmap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply method pd.corr() only to the numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_c_num_df = ca_c_df._get_numeric_data()\n",
    "correlation_matrix = ca_c_num_df.corr()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the heatmap with seaborn fuction sns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure and axes\n",
    "fig5, ax5 = plt.subplots(figsize = (8,8))\n",
    "\n",
    "# set title\n",
    "ax5.set_title('Correlation Heatmap', fontweight='bold')\n",
    "\n",
    "\n",
    "\n",
    "sns.heatmap(correlation_matrix,  # the data for the heatmap\n",
    "                          annot=True,  # show the actual values of correlation\n",
    "                          cmap='seismic',  # provide the 'seismic' colormap\n",
    "                          center=0,  # specify the value at which to center the colormap\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical and Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_c_num_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_c_cat_df = ca_c_df.select_dtypes(\"object\")\n",
    "ca_c_cat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Pandas\n",
    "\n",
    "ca_c_num_df.hist(figsize=(12,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using sns and dedicated function\n",
    "\n",
    "def dist_plot(df, ncolumns):\n",
    "\n",
    "    columns = np.array(df.columns)\n",
    "    nrows = (len(columns) % ncolumns) + 1\n",
    "\n",
    "    fig, axs = plt.subplots(nrows, ncolumns, figsize = (16,16))\n",
    "\n",
    "    pad_value = (nrows - (len(columns) // nrows))\n",
    "    columns = np.pad(columns, pad_value, constant_values = \"\")[pad_value:]\n",
    "    columns = columns.reshape(nrows, ncolumns)\n",
    "\n",
    "\n",
    "    for i in range(nrows):\n",
    "\n",
    "        for j in range(ncolumns):\n",
    "\n",
    "            try:\n",
    "\n",
    "                sns.histplot(df, x = columns[i,j], ax = axs[i, j])\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_plot(ca_c_num_df, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(ca_c_num_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using sns FacetGrid\n",
    "\n",
    "\n",
    "# First we melt the dataframe to have all data in one single column\n",
    "ca_c_num_df_melted = ca_c_num_df.melt(var_name=\"column\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_c_num_df_melted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_c_num_df_melted.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FacetGrid\n",
    "g = sns.FacetGrid(ca_c_num_df_melted, col='column', col_wrap=3, sharex=False, sharey=False)\n",
    "g.map(plt.hist, 'value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the numerical variables seems to be normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no high correlated feature pairs in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(ca_c_num_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## x-y split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we separate the dependent variable (Y) from the independent variables (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ca_c_df.drop(\"total_claim_amount\", axis=1)\n",
    "y = ca_c_df[\"total_claim_amount\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We allocate the categorical variable and the numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num = X._get_numeric_data()\n",
    "X_cat = X.select_dtypes(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-max normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num_scaled_minmax = pd.DataFrame(MinMaxScaler().fit_transform(X_num), columns=X_num.columns)\n",
    "X_num_scaled_minmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num_scaled_standard = pd.DataFrame(StandardScaler().fit_transform(X_num), columns=X_num.columns)\n",
    "X_num_scaled_standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Variables Enconding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat_ord = X_cat[[\"response\", \"coverage\", \"education\", \"renew_offer_type\", \"vehicle_size\"]]\n",
    "categories = [[\"no\",\"yes\"], [\"basic\", \"extended\", \"premium\"], [\"high school or below\", \"college\", \"bachelor\", \"master\", \"doctor\"], [\"offer1\", \"offer2\", \"offer3\", \"offer4\"], [\"not defined\", \"small\", \"medsize\", \"large\"]]\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder(categories=categories)\n",
    "X_cat_ord_encoded = pd.DataFrame(ordinal_encoder.fit_transform(X_cat_ord),columns=X_cat_ord.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat_ord_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding (For Nominal Variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat_nom = X_cat.drop(X_cat_ord.columns, axis=1)\n",
    "X_cat_nom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.get_dummies()\n",
    "X_cat_nom_encoded = pd.get_dummies(X_cat_nom,drop_first=True)\n",
    "\n",
    "#drop_first = True creates K-1 columns\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with sklearn\n",
    "\n",
    "X_cat_nom_encoded = pd.DataFrame(OneHotEncoder(drop=\"first\").fit_transform(X_cat_nom).toarray(),\n",
    "                             columns=OneHotEncoder(drop='first').fit(X_cat_nom).get_feature_names_out(input_features=X_cat_nom.columns))\n",
    "X_cat_nom_encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_transformations(df, test_size=0.2, scaler = \"minmax\"):\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "    test_size : float, optional\n",
    "        Size in fraction (0-1) of the test set. The default is 0.2.\n",
    "    scaler : String, optional\n",
    "        The type of numerical scaling method to be used. Values accepted are\n",
    "        \"standard\" and \"minmax\". The default is \"standard\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing two records, \"train\" and \"test\".\n",
    "        These two are lists containing the transformed train and test set:\n",
    "            - Scaled Numerical columns\n",
    "            - Encoded Categorical columns\n",
    "            - Target Variable\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # x-y split\n",
    "\n",
    "    X = df.drop(\"total_claim_amount\", axis=1)\n",
    "    y = df[\"total_claim_amount\"]\n",
    "\n",
    "    # Train/Test Split\n",
    "    X_train, X_test, y_train, y_test=train_test_split(X, y, test_size= test_size, random_state=0)\n",
    "    \n",
    "    \n",
    "    # Numerical and categorical variables\n",
    "\n",
    "    X_train_num = X_train._get_numeric_data()\n",
    "    X_test_num = X_test._get_numeric_data()\n",
    "    \n",
    "    X_train_cat = X_train.select_dtypes(\"category\")\n",
    "    X_test_cat = X_test.select_dtypes(\"category\")\n",
    "\n",
    "    # Categorical Variables Encoding - Ordinals\n",
    "\n",
    "    X_train_cat_ord = X_train_cat[[\"response\", \"coverage\", \"education\", \"renew_offer_type\", \"vehicle_size\"]]\n",
    "    X_test_cat_ord = X_test_cat[[\"response\", \"coverage\", \"education\", \"renew_offer_type\", \"vehicle_size\"]]\n",
    "\n",
    "    categories = [[\"no\",\"yes\"], [\"basic\", \"extended\", \"premium\"], [\"high school or below\", \"college\", \"bachelor\", \"master\", \"doctor\"], [\"offer1\", \"offer2\", \"offer3\", \"offer4\"], [\"not defined\", \"small\", \"medsize\", \"large\"]]\n",
    "    \n",
    "    ordinal_encoder = OrdinalEncoder(categories=categories)\n",
    "    X_train_cat_ord_encoded = pd.DataFrame(ordinal_encoder.fit_transform(X_train_cat_ord),columns=X_train_cat_ord.columns).reset_index(drop=True)\n",
    "    X_test_cat_ord_encoded = pd.DataFrame(ordinal_encoder.fit_transform(X_test_cat_ord),columns=X_test_cat_ord.columns).reset_index(drop=True)\n",
    "    \n",
    "    # Categorical Variables Encoding - Nominals\n",
    "\n",
    "    X_train_cat_nom = X_train_cat.drop(X_train_cat_ord.columns, axis=1)\n",
    "    X_test_cat_nom = X_test_cat.drop(X_test_cat_ord.columns, axis=1)\n",
    "\n",
    "    X_train_cat_nom_encoded = pd.get_dummies(X_train_cat_nom,drop_first=True).reset_index(drop=True)\n",
    "    X_test_cat_nom_encoded = pd.get_dummies(X_test_cat_nom,drop_first=True).reset_index(drop=True)\n",
    "    \n",
    "    # Concatinating back together the categorical features\n",
    "    X_train_cat = pd.concat([X_train_cat_nom_encoded, X_train_cat_ord_encoded], axis=1)\n",
    "    X_test_cat = pd.concat([X_test_cat_nom_encoded, X_test_cat_ord_encoded], axis=1)\n",
    "    \n",
    "    # Scaling of the Numerical Variables\n",
    "    if scaler != None:\n",
    "       \n",
    "       if scaler == \"minmax\":\n",
    "           \n",
    "           scaler = MinMaxScaler()\n",
    "           \n",
    "       elif scaler == \"standard\":\n",
    "           \n",
    "           scaler = StandardScaler()\n",
    "    \n",
    "       scaler.fit(X_train_num)\n",
    "       X_train_num_scaled = pd.DataFrame(scaler.transform(X_train_num), columns=X_train_num.columns).reset_index(drop=True)\n",
    "       X_test_num_scaled = pd.DataFrame(scaler.transform(X_test_num), columns=X_test_num.columns).reset_index(drop=True)\n",
    "    \n",
    "       return {\"train\":[X_train_num_scaled, X_train_cat, y_train], \"test\":[X_test_num_scaled, X_test_cat, y_test]}\n",
    "\n",
    "    return {\"train\":[X_train_num, X_train_cat, y_train], \"test\":[X_test_num, X_test_cat, y_test]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R2_adjusted(x, y, y_pred, R2=None):\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : DataFrame or Serie\n",
    "        Estimators\n",
    "    y : DataFrame or Serie\n",
    "        Observed Target Variable.\n",
    "    y_pred : DataFrame or Serie\n",
    "        Predicted Target Variable.\n",
    "    r2 : Float, optional\n",
    "        R2 value previously calculated. The default is None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    r2_adj : Float\n",
    "        Adjusted R2 value.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if R2==None:\n",
    "    \n",
    "        R2 = r2_score(y, y_pred)\n",
    "    \n",
    "    else:\n",
    "        R2_adj = 1 - (1-R2)*(len(y)-1)/(len(y)-X.shape[1]-1)\n",
    "\n",
    "    return R2_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prediction_plot(model, X, y, set=\"test\", y_transformer = None, verbose = True, plot = True):\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Sklearn model object\n",
    "        Model to be used for the prediction\n",
    "    X : DataFrame or Series\n",
    "        Estimators (train or test sets).\n",
    "    y : DataFrame or Series\n",
    "        Observed Target Variable (train or test sets).\n",
    "    y_transformer : String or sklearn transformer object, optional\n",
    "        The normalization transformer used on the target variable, if any. The default is None.\n",
    "    verbose : Boolean, optional\n",
    "        The default is True.\n",
    "    plot : Boolean, optional\n",
    "        The default is True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Results of the regression.\n",
    "        - Prediction results\n",
    "        - Performance Metrics\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    # Do the inverse normal transformation to return the dependent variable to meaningfull values\n",
    "    if y_transformer != None:\n",
    "\n",
    "        if y_transformer != \"log\":\n",
    "\n",
    "            y = pd.DataFrame(y_transformer.inverse_transform(np.array(y).reshape(-1, 1)))[0]\n",
    "            y_pred = pd.DataFrame(y_transformer.inverse_transform(y_pred.reshape(-1, 1)))[0]\n",
    "        \n",
    "        elif y_transformer == \"log\":\n",
    "\n",
    "            y = np.exp(y)\n",
    "            y_pred = np.exp(y_pred)\n",
    "\n",
    "\n",
    "    result = pd.DataFrame({f\"y_{set}\":y, \"y_pred\":y_pred})\n",
    "\n",
    "    # Metrics\n",
    "    \n",
    "    R2 = r2_score(y, y_pred)\n",
    "    RMSE = mse(y, y_pred, squared=False)\n",
    "    R2_adj = R2_adjusted(X, y, y_pred, R2)\n",
    "    metrics = {\"MSE\":mse(y, y_pred), \"RMSE\":RMSE, \"MAE\":mae(y, y_pred), \"R2\":R2, \"R2_adj\":R2_adj}\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "\n",
    "        print(f\"The model performance for the {set} set\")\n",
    "        print(\"-------------------------------------------\")\n",
    "        print(f\"RMSE of {set} set is {RMSE}\")\n",
    "        print(f\"Adjusted R2 score of {set} set is {R2_adj}\")\n",
    "        print()\n",
    "\n",
    "    # Plots\n",
    "\n",
    "    if plot:\n",
    "\n",
    "        fig, axs = plt.subplots(1,3,figsize=(15,5))\n",
    "\n",
    "        fs = 11\n",
    "    \n",
    "        plt.xticks(fontsize = fs)\n",
    "\n",
    "        sns.regplot(x=f\"y_{set}\", y=\"y_pred\", data=result, scatter_kws={\"color\": \"red\"}, line_kws={\"color\": \"black\"}, ax=axs[0])\n",
    "        sns.histplot(y-y_pred, kde=True, ax=axs[1])\n",
    "        axs[2].plot(y_pred,y - y_pred,\"o\")\n",
    "        axs[2].plot(y_pred,np.zeros(len(y_pred)),linestyle='dashed')\n",
    "\n",
    "        axs[0].set_title(f\"y_{set}\".capitalize() + \" Set - Observed VS Predicted\", fontsize = fs)\n",
    "        axs[1].set_title(f\"y_{set}\".capitalize() + \" Set - Histogram of the Residuals\", fontsize = fs)\n",
    "        axs[1].set_xlabel(f\"y_{set}\" + \" - y_pred\", fontsize = fs)\n",
    "        axs[2].set_xlabel(\"Predicted\", fontsize = fs)\n",
    "        axs[2].set_ylabel(\"Residuals\", fontsize = fs)\n",
    "        axs[2].set_title(\"Residuals by Predicted\", fontsize = fs)\n",
    "\n",
    "    \n",
    "\n",
    "    return {\"Result\":result, \"Metrics\":metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_flex_regression(X_train, y_train, X_test, y_test, model, verbose = True, plot = True, y_transformer = None):\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : DataFrame or Series\n",
    "        Estimators (train set).\n",
    "    y_train : DataFrame or Series\n",
    "        Observed Target Variable (train set).\n",
    "    X_test : DataFrame or Series\n",
    "        Estimators (test set).\n",
    "    y_test : DataFrame or Series\n",
    "        Observed Target Variable (test set).\n",
    "    model : Sklearn model object\n",
    "        Model to be used for the prediction\n",
    "    verbose : Boolean, optional\n",
    "        The default is True.\n",
    "    plot : Boolean, optional\n",
    "        The default is True.\n",
    "    y_transformer : String or sklearn transformer object, optional\n",
    "        The normalization transformer used on the target variable, if any. The default is None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Results of the regression.\n",
    "        - Fitted Model\n",
    "        - Prediction results\n",
    "        - Performance Metrics\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Model Fit\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluation on the Train Set\n",
    "    train = prediction_plot(model, X_train, y_train, set=\"train\", y_transformer = y_transformer, verbose = verbose, plot = plot)\n",
    "\n",
    "    # Evaluation on the Test Set\n",
    "    test = prediction_plot(model, X_test, y_test, set=\"test\", y_transformer = y_transformer, verbose = verbose, plot = plot)\n",
    "\n",
    "    dict_for_df = {\"train\": train[\"Metrics\"], \"test\": test[\"Metrics\"]}\n",
    "    df_metrics = pd.DataFrame(dict_for_df)\n",
    "\n",
    "    return {\"Model\":model, \"Metrics\": df_metrics, \"Train\":train[\"Result\"], \"Test\":test[\"Result\"]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the data is loaded. For naming consistency the column headings are standarized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_c_df = pd.read_csv(\"Data/Data_Marketing_Customer_Analysis_Round2.csv\")\n",
    "ca_c_df = standard_headings(ca_c_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables to set up the cleaning pipeline are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = [\"unnamed:_0\", \"customer\", \"effective_to_date\", \"policy\", \"vehicle_type\"]\n",
    "bucket_columns = [\"vehicle_class\"]\n",
    "clean_columns_zeros = [\"income\"]\n",
    "clean_columns_cat = [(\"response\", \"No\")]\n",
    "float_columns = list(set(ca_c_df._get_numeric_data().columns).difference(set(drop_columns)))\n",
    "obj_columns = list(set(ca_c_df.select_dtypes(\"object\").columns).difference(set(drop_columns)))\n",
    "\n",
    "cleaning_dict = {\"Luxury SUV\":\"Luxury\", \"Luxury Car\":\"Luxury\"\n",
    "                 ,np.nan:np.nan\n",
    "                 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaning pipeline is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_c_df = (pd.read_csv(\"Data/Data_Marketing_Customer_Analysis_Round2.csv\")\n",
    "             .pipe(standard_headings)\n",
    "             .drop(drop_columns, axis=1)\n",
    "             .pipe(bucket_series, bucket_columns ,cleaning_dict)\n",
    "             .pipe(replace_by, clean_columns_zeros, replace_zeros = True)\n",
    "             .pipe(replace_by_cat, clean_columns_cat)\n",
    "             .dropna()\n",
    "             .pipe(standardize_data)\n",
    "             .pipe(float_to_int, float_columns)\n",
    "             .pipe(obj_to_cat, obj_columns)\n",
    "             .drop_duplicates()\n",
    "             .reset_index(drop=True)\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_c_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_counter(ca_c_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model - only numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Standard Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is transformed using user-defined function my_transformations():\n",
    "- X/Y Slpit\n",
    "- Train/Test Split\n",
    "- Scaling of the numerical features (Standard Scaler or MinMax Scaler)\n",
    "- Encoding of the categorical features (ordinal or nominal where appropiate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = my_transformations(ca_c_df, test_size=0.2, scaler=\"standard\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = transformed[\"train\"][0]\n",
    "y_train = transformed[\"train\"][2]\n",
    "\n",
    "X_test = transformed[\"test\"][0]\n",
    "y_test = transformed[\"test\"][2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression model is set-up, trained, analyzed and interpreted using user-defined function my_flex_regression()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_baseline = my_flex_regression(X_train, y_train, X_test, y_test, LinearRegression(), y_transformer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_baseline[\"Metrics\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error seems to follow a normal distribution. Nevertheless it is too high and can be reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = list(zip(X_train.columns, output_baseline[\"Model\"].coef_))\n",
    "feature_importance = sorted(feature_importance, key=lambda x:abs(x[1]), reverse=True)\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monthly Premium Auto is the more relevant numerical feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Min-Max Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = my_transformations(ca_c_df, test_size=0.2, scaler=\"minmax\")\n",
    "\n",
    "X_train = transformed[\"train\"][0]\n",
    "y_train = transformed[\"train\"][2]\n",
    "\n",
    "X_test = transformed[\"test\"][0]\n",
    "y_test = transformed[\"test\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_baseline = my_flex_regression(X_train, y_train, X_test, y_test, LinearRegression(), y_transformer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_baseline[\"Metrics\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The scaling method has no impact in this case**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including All Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = my_transformations(ca_c_df, test_size=0.2, scaler=\"minmax\")\n",
    "\n",
    "# The numerical and the categorical features are concatenated back together\n",
    "X_train = pd.concat([transformed[\"train\"][0], transformed[\"train\"][1]], axis=1)\n",
    "y_train = transformed[\"train\"][2]\n",
    "\n",
    "X_test = pd.concat([transformed[\"test\"][0], transformed[\"test\"][1]], axis=1)\n",
    "y_test = transformed[\"test\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_all_cat = my_flex_regression(X_train, y_train, X_test, y_test, LinearRegression(), y_transformer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_all_cat[\"Metrics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = list(zip(X_train.columns, output_all_cat[\"Model\"].coef_))\n",
    "feature_importance = sorted(feature_importance, key=lambda x:abs(x[1]), reverse=True)\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Improvements - Removing Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest coefficient corresponds to \"Monthly Premium Auto\". This feature therefore is the one that contributes the more to \"Total Claim Amount\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_one_iteration(df, column):\n",
    "\n",
    "    pct_75 = pct(df[column], 75)\n",
    "    pct_25 = pct(df[column], 25)\n",
    "    upper_bound = pct_75 + 1.5*iqr(df[column])\n",
    "    lower_bound = pct_25 - 1.5*iqr(df[column])\n",
    "    df = df[(df[column] <= upper_bound) & (df[column] >= lower_bound)]\n",
    "    new_upper_bound = pct_75 + 1.5*iqr(df[column])\n",
    "    new_lower_bound = pct_25 - 1.5*iqr(df[column])\n",
    "    remaining_outliers = len(df[(df[column] > new_upper_bound) | (df[column] < new_lower_bound)])\n",
    "\n",
    "    return df, remaining_outliers\n",
    "\n",
    "def remove_outliers(df, columns_list):\n",
    "\n",
    "    for column in columns_list:\n",
    "\n",
    "        remaining_outliers = 1\n",
    "        iter=0\n",
    "\n",
    "        while remaining_outliers > 0:\n",
    "\n",
    "            df, remaining_outliers = remove_outliers_one_iteration(df, column)\n",
    "            \n",
    "            iter += 1\n",
    "            print((column, iter, remaining_outliers))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_c_no_outliers_df = (remove_outliers(ca_c_df, ['customer_lifetime_value', 'income', 'monthly_premium_auto', \"total_claim_amount\"])\n",
    "                       .reset_index(drop=True)\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The number of rows before removing outliers is {len(ca_c_df)}\")\n",
    "print(f\"The number of rows after removing outliers is {len(ca_c_no_outliers_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the canvas\n",
    "\n",
    "fig, axs = plt.subplots(3,2,figsize=(16,16))\n",
    "\n",
    "fig.suptitle(\"Outliers - Before and After\")\n",
    "\n",
    "# Monthly Premium Auto\n",
    "sns.boxplot(data=ca_c_df, x=\"monthly_premium_auto\", ax=axs[0,0])\n",
    "sns.boxplot(data=ca_c_no_outliers_df, x=\"monthly_premium_auto\", ax=axs[0,1])\n",
    "\n",
    "# Total claim amount\n",
    "sns.boxplot(data=ca_c_df, x=\"total_claim_amount\", ax=axs[1,0])\n",
    "sns.boxplot(data=ca_c_no_outliers_df, x=\"total_claim_amount\", ax=axs[1,1])\n",
    "\n",
    "# Customer Lifetime Value\n",
    "sns.boxplot(data=ca_c_df, x=\"customer_lifetime_value\", ax=axs[2,0])\n",
    "sns.boxplot(data=ca_c_no_outliers_df, x=\"customer_lifetime_value\", ax=axs[2,1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = my_transformations(ca_c_no_outliers_df, test_size=0.2, scaler=\"minmax\")\n",
    "\n",
    "X_train = pd.concat([transformed[\"train\"][0], transformed[\"train\"][1]], axis=1)\n",
    "y_train = transformed[\"train\"][2]\n",
    "\n",
    "X_test = pd.concat([transformed[\"test\"][0], transformed[\"test\"][1]], axis=1)\n",
    "y_test = transformed[\"test\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_all_cat_no_outliers = my_flex_regression(X_train, y_train, X_test, y_test, LinearRegression(), y_transformer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_all_cat_no_outliers[\"Metrics\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_const = sm.add_constant(X_train)  \n",
    "X_test_const = sm.add_constant(X_test)\n",
    "\n",
    "model = sm.OLS(y_train, X_train_const).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Transform on Monthly Premium Amount?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_c_no_outliers_df.hist(figsize=(12,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(ca_c_no_outliers_df[\"monthly_premium_auto\"], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(ca_c_no_outliers_df[\"total_claim_amount\"], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = PowerTransformer()\n",
    "\n",
    "monthly_premium_auto_transformer = pt.fit(ca_c_no_outliers_df[\"monthly_premium_auto\"].to_numpy().reshape(-1,1))\n",
    "\n",
    "monthly_premium_auto_transformed = monthly_premium_auto_transformer.transform(ca_c_no_outliers_df[\"monthly_premium_auto\"].to_numpy().reshape(-1,1))\n",
    "\n",
    "print(monthly_premium_auto_transformer.lambdas_)\n",
    "monthly_premium_auto_transformed = pd.DataFrame(monthly_premium_auto_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_claim_amount_transformer = pt.fit(ca_c_no_outliers_df[\"total_claim_amount\"].to_numpy().reshape(-1,1))\n",
    "\n",
    "total_claim_amount_transformed = total_claim_amount_transformer.transform(ca_c_no_outliers_df[\"total_claim_amount\"].to_numpy().reshape(-1,1))\n",
    "\n",
    "print(total_claim_amount_transformer.lambdas_)\n",
    "total_claim_amount_transformed = pd.DataFrame(total_claim_amount_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(monthly_premium_auto_transformed, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(total_claim_amount_transformed, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_c_transf_df = ca_c_no_outliers_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ca_c_transf_df[\"monthly_premium_auto\"] = monthly_premium_auto_transformed\n",
    "# ca_c_transf_df[\"total_claim_amount\"] = total_claim_amount_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = my_transformations(ca_c_transf_df, test_size=0.2, scaler=\"minmax\")\n",
    "\n",
    "X_train = pd.concat([transformed[\"train\"][0], transformed[\"train\"][1]], axis=1)\n",
    "y_train = transformed[\"train\"][2]\n",
    "\n",
    "X_test = pd.concat([transformed[\"test\"][0], transformed[\"test\"][1]], axis=1)\n",
    "y_test = transformed[\"test\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = PowerTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_premium_auto_transformer = pt.fit(X_train[\"monthly_premium_auto\"].to_numpy().reshape(-1,1))\n",
    "\n",
    "X_train[\"monthly_premium_auto\"] = monthly_premium_auto_transformer.transform(X_train[\"monthly_premium_auto\"].to_numpy().reshape(-1,1))\n",
    "X_test[\"monthly_premium_auto\"] = monthly_premium_auto_transformer.transform(X_test[\"monthly_premium_auto\"].to_numpy().reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_claim_amount_transformer = pt.fit(y_train.to_numpy().reshape(-1,1))\n",
    "\n",
    "y_train = pd.DataFrame(total_claim_amount_transformer.transform(y_train.to_numpy().reshape(-1,1)))\n",
    "y_test = pd.DataFrame(total_claim_amount_transformer.transform(y_test.to_numpy().reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_all_cat_no_outliers_transformed = my_flex_regression(X_train, y_train, X_test, y_test, LinearRegression(), y_transformer=total_claim_amount_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_all_cat_no_outliers_transformed[\"Metrics\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The adjusted R2 for the baseline model (only numerical features) is {round(output_baseline[\"Metrics\"].loc[\"R2_adj\",\"test\"],2)}')\n",
    "print(f'The adjusted R2 for the model baseline + all categorical features is {round(output_all_cat[\"Metrics\"].loc[\"R2_adj\",\"test\"],2)}')\n",
    "print(f'The adjusted R2 for the model baseline + all categorical features + removed outliers is {round(output_all_cat_no_outliers[\"Metrics\"].loc[\"R2_adj\",\"test\"],2)}')\n",
    "print(f'The adjusted R2 for the model baseline + all categorical features + removed outliers + normalization is {round(output_all_cat_no_outliers_transformed[\"Metrics\"].loc[\"R2_adj\",\"test\"],2)}')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "350fa199ac5972690c7e339cc739f563045f32da9cdb6c3e3b5da606ff6ebbd1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "244px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
